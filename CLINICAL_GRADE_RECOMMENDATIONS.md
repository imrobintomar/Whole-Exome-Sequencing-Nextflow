# Clinical-Grade WES Pipeline Recommendations
## For SaaS-Based Genomics Analysis Platform

**Date:** 2026-01-14
**Pipeline Version:** 1.0
**Review Type:** Production Readiness Assessment

---

## Executive Summary

Your Nextflow Whole Exome Sequencing (WES) pipeline demonstrates **strong architectural design** with robust error handling, dynamic resource allocation, and comprehensive backend integration. However, for clinical-grade SaaS deployment, **critical enhancements are required** in the areas of:

- **Regulatory Compliance** (HIPAA, CLIA, CAP, GDPR)
- **Variant Quality Control** (VQSR, hard filtering)
- **Clinical Reporting** (PDF reports, ACMG secondary findings)
- **Security & Audit Logging** (comprehensive audit trails)
- **Data Validation** (input validation, contamination detection)

**Current Status:** ‚úÖ Research-Ready | ‚ö†Ô∏è Clinical-Ready (with enhancements)

---

## üî¥ CRITICAL ISSUES (Must Fix Before Production)

### 1. Remove Author Attribution Blocking Mechanism

**Current Code (main.nf:38-63):**
```groovy
if (workflow.manifest.author != requiredAuthor || workflow.manifest.homePage != requiredGithub) {
    error "PIPELINE EXECUTION BLOCKED - Author attribution tampered!"
}
```

**Problem:**
- Prevents legitimate code improvements, bug fixes, and institutional modifications
- Blocks compliance teams from adding required validations
- Incompatible with collaborative development and forking

**Solution:**
```groovy
// Replace with warning instead of blocking error
if (workflow.manifest.author != requiredAuthor || workflow.manifest.homePage != requiredGithub) {
    log.warn """
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    NOTICE: Pipeline source information has been modified
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    Original Author:  ${requiredAuthor}
    Original GitHub:  ${requiredGithub}

    If you are using this pipeline in production, please ensure:
    - All modifications are validated
    - Compliance requirements are met
    - Appropriate testing has been performed
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    """
}
```

**Impact:** Unblocks institutional adoption, allows quality improvements

---

### 2. Implement Variant Quality Score Recalibration (VQSR)

**Current Limitation:**
- HaplotypeCaller outputs all variants without quality-based recalibration
- Simple depth/allele frequency filtering only
- Higher false positive rate compared to GATK best practices

**Solution - Add VQSR Process:**

Create `/processes/07b_vqsr.nf`:
```groovy
process variantRecalibrator {
    tag "$sample_id"

    publishDir "${params.output_dir}/VQSR", mode: 'copy'

    input:
        tuple val(sample_id), path(raw_vcf), path(raw_vcf_idx)

    output:
        tuple val(sample_id), path("${sample_id}.recal.vcf.gz"), path("${sample_id}.recal.vcf.gz.tbi")

    script:
    def avail_mem_mb = (task.memory.toMega() * 0.9).toInteger()
    """
    # SNP Recalibration
    gatk --java-options "-Xmx${avail_mem_mb}m" VariantRecalibrator \\
        -R ${params.reference} \\
        -V ${raw_vcf} \\
        --resource:hapmap,known=false,training=true,truth=true,prior=15.0 \\
            ${params.hapmap_vcf} \\
        --resource:omni,known=false,training=true,truth=false,prior=12.0 \\
            ${params.omni_vcf} \\
        --resource:1000G,known=false,training=true,truth=false,prior=10.0 \\
            ${params.phase1_snps} \\
        --resource:dbsnp,known=true,training=false,truth=false,prior=2.0 \\
            ${params.dbsnp_vcf} \\
        -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR \\
        -mode SNP \\
        -O ${sample_id}.snps.recal \\
        --tranches-file ${sample_id}.snps.tranches \\
        --rscript-file ${sample_id}.snps.plots.R

    # Apply SNP Recalibration
    gatk --java-options "-Xmx${avail_mem_mb}m" ApplyVQSR \\
        -R ${params.reference} \\
        -V ${raw_vcf} \\
        --recal-file ${sample_id}.snps.recal \\
        --tranches-file ${sample_id}.snps.tranches \\
        -mode SNP \\
        --truth-sensitivity-filter-level 99.0 \\
        -O ${sample_id}.snps_recal.vcf.gz

    # INDEL Recalibration
    gatk --java-options "-Xmx${avail_mem_mb}m" VariantRecalibrator \\
        -R ${params.reference} \\
        -V ${sample_id}.snps_recal.vcf.gz \\
        --resource:mills,known=false,training=true,truth=true,prior=12.0 \\
            ${params.mills_indels} \\
        --resource:dbsnp,known=true,training=false,truth=false,prior=2.0 \\
            ${params.dbsnp_vcf} \\
        -an QD -an MQRankSum -an ReadPosRankSum -an FS -an SOR \\
        -mode INDEL \\
        -O ${sample_id}.indels.recal \\
        --tranches-file ${sample_id}.indels.tranches

    # Apply INDEL Recalibration
    gatk --java-options "-Xmx${avail_mem_mb}m" ApplyVQSR \\
        -R ${params.reference} \\
        -V ${sample_id}.snps_recal.vcf.gz \\
        --recal-file ${sample_id}.indels.recal \\
        --tranches-file ${sample_id}.indels.tranches \\
        -mode INDEL \\
        --truth-sensitivity-filter-level 99.0 \\
        -O ${sample_id}.recal.vcf.gz

    # Index final VCF
    tabix -p vcf ${sample_id}.recal.vcf.gz

    echo "‚úì VQSR completed: ${sample_id}.recal.vcf.gz"
    """
}
```

**Update main.nf:**
```groovy
include { variantRecalibrator } from './processes/07b_vqsr.nf'

// After haplotypeCaller:
vcf_raw = haplotypeCaller(final_bam)
vcf_recalibrated = variantRecalibrator(vcf_raw)  // NEW
annovar_txt = annovarAnnotate(vcf_recalibrated)  // Use recalibrated VCF
```

**Required Reference Files (add to nextflow.config):**
```groovy
params {
    hapmap_vcf    = "/path/to/hapmap_3.3.hg38.vcf.gz"
    omni_vcf      = "/path/to/1000G_omni2.5.hg38.vcf.gz"
    phase1_snps   = "/path/to/1000G_phase1.snps.high_confidence.hg38.vcf.gz"
    dbsnp_vcf     = "/path/to/Homo_sapiens_assembly38.dbsnp138.vcf.gz"
    mills_indels  = "/path/to/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz"
}
```

**Impact:** Reduces false positives by 30-50%, aligns with GATK best practices

---

### 3. Add FASTQ Input Validation

**Current Limitation:**
- No pre-flight FASTQ validation
- Malformed input causes pipeline failure after hours of processing
- No detection of non-FASTQ files

**Solution - Create Validation Process:**

Create `/processes/00_validation.nf`:
```groovy
process validateFASTQ {
    tag "$sample_id"

    input:
        tuple val(sample_id), path(read1), path(read2)

    output:
        tuple val(sample_id), path(read1), path(read2)

    errorStrategy 'terminate'

    script:
    """
    #!/usr/bin/env python3
    import gzip
    import sys

    def validate_fastq(filepath, max_reads=10000):
        """Validate FASTQ format"""
        try:
            opener = gzip.open if filepath.endswith('.gz') else open
            mode = 'rt' if filepath.endswith('.gz') else 'r'

            with opener(filepath, mode) as f:
                for i, line in enumerate(f):
                    if i >= max_reads * 4:
                        break

                    # Check header line (@)
                    if i % 4 == 0 and not line.startswith('@'):
                        print(f"ERROR: Invalid header at line {i+1}: {line[:50]}")
                        return False

                    # Check quality line (+)
                    if i % 4 == 2 and not line.startswith('+'):
                        print(f"ERROR: Invalid quality header at line {i+1}")
                        return False

                    # Check sequence and quality length match
                    if i % 4 == 1:
                        seq_len = len(line.strip())
                    if i % 4 == 3:
                        qual_len = len(line.strip())
                        if seq_len != qual_len:
                            print(f"ERROR: Seq/Qual length mismatch at line {i+1}")
                            return False

            return True
        except Exception as e:
            print(f"ERROR: Failed to read {filepath}: {e}")
            return False

    # Validate both reads
    print("Validating ${read1}...")
    if not validate_fastq("${read1}"):
        sys.exit(1)

    print("Validating ${read2}...")
    if not validate_fastq("${read2}"):
        sys.exit(1)

    print("‚úì FASTQ validation passed for ${sample_id}")
    """
}
```

**Update main.nf:**
```groovy
include { validateFASTQ } from './processes/00_validation.nf'

// After read_pairs channel creation:
read_pairs = Channel.fromFilePairs("${params.input_dir}/*_{1,2}.fastq.gz", size: 2)
validated_pairs = validateFASTQ(read_pairs)  // NEW
fastp_out = fastpQC(validated_pairs)         // Use validated input
```

**Impact:** Prevents wasted compute time, improves error messages

---

### 4. Implement Comprehensive Audit Logging (HIPAA Compliance)

**Current Limitation:**
- Limited audit trail
- No user action tracking
- Insufficient for regulatory compliance

**Solution - Add Audit Logging System:**

Create `/backend/models/audit_log.py`:
```python
from sqlalchemy import Column, Integer, String, DateTime, Text, JSON
from datetime import datetime
from database import Base

class AuditLog(Base):
    __tablename__ = "audit_logs"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(String, index=True)
    user_email = Column(String, index=True)
    action = Column(String(100), index=True)
    resource_type = Column(String(50))  # job, file, user, config
    resource_id = Column(String(100))
    details = Column(JSON)
    ip_address = Column(String(45))
    user_agent = Column(String(500))
    timestamp = Column(DateTime, default=datetime.utcnow, index=True)

    # Compliance fields
    phi_accessed = Column(Boolean, default=False)  # Protected Health Info
    data_exported = Column(Boolean, default=False)

    def __repr__(self):
        return f"<AuditLog {self.user_email} {self.action} at {self.timestamp}>"
```

**Create `/backend/services/audit_service.py`:**
```python
from models.audit_log import AuditLog
from sqlalchemy.orm import Session
from datetime import datetime
from fastapi import Request

class AuditService:

    ACTIONS = {
        # Authentication
        "USER_LOGIN": "User logged in",
        "USER_LOGOUT": "User logged out",
        "AUTH_FAILED": "Authentication failed",

        # Job Management
        "JOB_CREATED": "Analysis job created",
        "JOB_STARTED": "Analysis job started",
        "JOB_COMPLETED": "Analysis job completed",
        "JOB_FAILED": "Analysis job failed",
        "JOB_CANCELLED": "Analysis job cancelled",

        # Data Access
        "FILE_UPLOADED": "FASTQ file uploaded",
        "FILE_DOWNLOADED": "Result file downloaded",
        "RESULTS_VIEWED": "Analysis results viewed",
        "VCF_ACCESSED": "VCF file accessed",
        "BAM_ACCESSED": "BAM file accessed",

        # Admin Actions
        "USER_BANNED": "User account banned",
        "USER_UNBANNED": "User account unbanned",
        "CONFIG_CHANGED": "System configuration changed",
        "FILTER_APPLIED": "Variant filter applied",

        # Compliance
        "DATA_EXPORTED": "PHI data exported",
        "DATA_DELETED": "PHI data deleted",
        "CONSENT_UPDATED": "User consent updated",
    }

    @staticmethod
    def log(
        db: Session,
        user_id: str,
        user_email: str,
        action: str,
        resource_type: str = None,
        resource_id: str = None,
        details: dict = None,
        request: Request = None,
        phi_accessed: bool = False,
        data_exported: bool = False
    ):
        """Create audit log entry"""

        audit_entry = AuditLog(
            user_id=user_id,
            user_email=user_email,
            action=action,
            resource_type=resource_type,
            resource_id=resource_id,
            details=details or {},
            ip_address=request.client.host if request else None,
            user_agent=request.headers.get("user-agent") if request else None,
            phi_accessed=phi_accessed,
            data_exported=data_exported,
            timestamp=datetime.utcnow()
        )

        db.add(audit_entry)
        db.commit()

        return audit_entry

    @staticmethod
    def get_user_activity(db: Session, user_id: str, limit: int = 100):
        """Retrieve user's recent activity"""
        return db.query(AuditLog).filter(
            AuditLog.user_id == user_id
        ).order_by(AuditLog.timestamp.desc()).limit(limit).all()

    @staticmethod
    def get_phi_access_logs(db: Session, days: int = 30):
        """Get all PHI access logs for compliance reporting"""
        from datetime import timedelta
        cutoff = datetime.utcnow() - timedelta(days=days)

        return db.query(AuditLog).filter(
            AuditLog.phi_accessed == True,
            AuditLog.timestamp >= cutoff
        ).order_by(AuditLog.timestamp.desc()).all()
```

**Update All Endpoints with Audit Logging:**

Example for job submission (`/backend/main.py`):
```python
from services.audit_service import AuditService

@app.post("/jobs/submit")
async def submit_job(
    request: Request,
    files: UploadFile = File(...),
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    # Create job
    job = create_job(...)

    # AUDIT LOG
    AuditService.log(
        db=db,
        user_id=user.firebase_uid,
        user_email=user.email,
        action="JOB_CREATED",
        resource_type="job",
        resource_id=job.job_id,
        details={
            "sample_name": job.sample_name,
            "fastq_files": [f.filename for f in files]
        },
        request=request,
        phi_accessed=False
    )

    return {"job_id": job.job_id}

@app.get("/jobs/{job_id}/results")
async def get_results(
    job_id: str,
    request: Request,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    # Retrieve results
    results = get_job_results(job_id)

    # AUDIT LOG - PHI ACCESS
    AuditService.log(
        db=db,
        user_id=user.firebase_uid,
        user_email=user.email,
        action="RESULTS_VIEWED",
        resource_type="job",
        resource_id=job_id,
        details={"variant_count": len(results.variants)},
        request=request,
        phi_accessed=True  # Results contain genetic data
    )

    return results
```

**Add Audit Log Export Endpoint:**
```python
@app.get("/admin/audit-logs/export")
async def export_audit_logs(
    start_date: str,
    end_date: str,
    admin_user: User = Depends(require_admin),
    db: Session = Depends(get_db)
):
    """Export audit logs for compliance reporting"""

    logs = db.query(AuditLog).filter(
        AuditLog.timestamp.between(start_date, end_date)
    ).all()

    # Convert to CSV
    import csv
    from io import StringIO

    output = StringIO()
    writer = csv.DictWriter(output, fieldnames=[
        'timestamp', 'user_email', 'action', 'resource_type',
        'resource_id', 'ip_address', 'phi_accessed'
    ])
    writer.writeheader()

    for log in logs:
        writer.writerow({
            'timestamp': log.timestamp.isoformat(),
            'user_email': log.user_email,
            'action': log.action,
            'resource_type': log.resource_type,
            'resource_id': log.resource_id,
            'ip_address': log.ip_address,
            'phi_accessed': log.phi_accessed
        })

    # AUDIT THIS ACTION TOO
    AuditService.log(
        db=db,
        user_id=admin_user.firebase_uid,
        user_email=admin_user.email,
        action="AUDIT_LOGS_EXPORTED",
        details={"date_range": f"{start_date} to {end_date}"},
        data_exported=True
    )

    return StreamingResponse(
        iter([output.getvalue()]),
        media_type="text/csv",
        headers={"Content-Disposition": "attachment; filename=audit_logs.csv"}
    )
```

**Impact:** Meets HIPAA audit requirements, enables compliance reporting

---

### 5. Fix Firebase Credentials Security Risk

**Current Code (backend/firebase_auth.py:13-14):**
```python
SERVICE_ACCOUNT_PATH = "./firebase-service-account.json"
cred = credentials.Certificate(SERVICE_ACCOUNT_PATH)
```

**Problem:**
- Hardcoded path risks credential exposure
- May be committed to version control
- Not configurable across environments

**Solution:**

**Update `/backend/firebase_auth.py`:**
```python
import os
from pathlib import Path

# Use environment variable with secure fallback
SERVICE_ACCOUNT_PATH = os.getenv(
    "GOOGLE_APPLICATION_CREDENTIALS",
    str(Path(__file__).parent / "firebase-service-account.json")
)

# Validate credentials file exists
if not Path(SERVICE_ACCOUNT_PATH).exists():
    raise FileNotFoundError(
        f"Firebase credentials not found at {SERVICE_ACCOUNT_PATH}. "
        "Please set GOOGLE_APPLICATION_CREDENTIALS environment variable."
    )

cred = credentials.Certificate(SERVICE_ACCOUNT_PATH)
firebase_admin.initialize_app(cred)
```

**Update `.gitignore`:**
```gitignore
# Firebase credentials
firebase-service-account.json
*service-account*.json

# Environment files
.env
.env.local
.env.production
```

**Create `.env.template`:**
```bash
# Firebase Authentication
GOOGLE_APPLICATION_CREDENTIALS=/path/to/firebase-service-account.json

# Database
DATABASE_URL=sqlite:///./wes_pipeline.db

# Email Configuration
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=your-email@gmail.com
SMTP_PASSWORD=your-app-password

# System Paths
REFERENCE_GENOME=/media/drprabudh/m1/hg38/hg38.fa
ANNOVAR_DB=/media/drprabudh/m1/annovar/hg38_humandb
```

**Impact:** Prevents credential leakage, improves security posture

---

### 6. Add Contamination Detection

**Current Limitation:**
- No detection of cross-sample contamination
- No species verification
- Risk of reporting incorrect results

**Solution - Add FastQ Screen:**

Create `/processes/00b_contamination.nf`:
```groovy
process fastqScreen {
    tag "$sample_id"

    publishDir "${params.output_dir}/QC/contamination", mode: 'copy'

    input:
        tuple val(sample_id), path(read1), path(read2)

    output:
        tuple val(sample_id), path("${sample_id}_screen.txt"), path("${sample_id}_screen.png")
        path("${sample_id}.contamination_check.pass") optional true

    script:
    """
    # Run FastQ Screen for contamination detection
    fastq_screen \\
        --aligner bowtie2 \\
        --conf ${params.fastq_screen_config} \\
        --threads ${task.cpus} \\
        --outdir . \\
        ${read1}

    # Parse results
    HUMAN_PCT=\$(grep "Human" ${sample_id}_screen.txt | awk '{print \$4}' | sed 's/%//')

    # Validate > 95% human reads
    if (( \$(echo "\$HUMAN_PCT > 95" | bc -l) )); then
        echo "‚úì Contamination check passed: \${HUMAN_PCT}% human reads"
        touch ${sample_id}.contamination_check.pass
    else
        echo "‚ö† WARNING: Only \${HUMAN_PCT}% human reads detected!"
        echo "Check for contamination or sample swap"
        exit 1
    fi
    """
}
```

**Add to nextflow.config:**
```groovy
params {
    fastq_screen_config = "/path/to/fastq_screen.conf"
}
```

**Create FastQ Screen Config (`fastq_screen.conf`):**
```ini
# FastQ Screen Configuration
DATABASE Human /path/to/bowtie2_index/hg38
DATABASE Mouse /path/to/bowtie2_index/mm10
DATABASE Rat /path/to/bowtie2_index/rn6
DATABASE Bacteria /path/to/bowtie2_index/bacteria
DATABASE Virus /path/to/bowtie2_index/virus
DATABASE Adapter /path/to/bowtie2_index/adapters
```

**Impact:** Detects sample swaps, contamination, quality issues early

---

## üü° HIGH PRIORITY ENHANCEMENTS

### 7. Implement Clinical Report Generation

**Current Gap:** Pipeline outputs TSV file only, no clinical interpretation

**Solution - Create PDF Report Generator:**

Create `/backend/services/report_generator.py`:
```python
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from datetime import datetime
import pandas as pd

class ClinicalReportGenerator:

    def __init__(self, job_id: str, user_info: dict, variants_df: pd.DataFrame):
        self.job_id = job_id
        self.user_info = user_info
        self.variants = variants_df
        self.styles = getSampleStyleSheet()

    def generate_report(self, output_path: str):
        """Generate comprehensive clinical report"""

        doc = SimpleDocTemplate(output_path, pagesize=letter)
        story = []

        # Title Page
        story.extend(self._create_title_page())
        story.append(PageBreak())

        # Executive Summary
        story.extend(self._create_executive_summary())
        story.append(PageBreak())

        # Pathogenic/Likely Pathogenic Variants
        story.extend(self._create_pathogenic_section())

        # Variants of Uncertain Significance
        story.extend(self._create_vus_section())

        # Secondary Findings (ACMG)
        story.extend(self._create_secondary_findings())

        # Methodology
        story.extend(self._create_methodology())

        # Build PDF
        doc.build(story)
        return output_path

    def _create_title_page(self):
        """Create title page with header"""
        elements = []

        title_style = ParagraphStyle(
            'CustomTitle',
            parent=self.styles['Heading1'],
            fontSize=24,
            textColor=colors.HexColor('#1f4788'),
            spaceAfter=30,
            alignment=1  # Center
        )

        elements.append(Paragraph("Whole Exome Sequencing Report", title_style))
        elements.append(Spacer(1, 0.5*inch))

        # Patient info table
        info_data = [
            ["Report ID:", self.job_id],
            ["Patient Name:", self.user_info.get('name', 'N/A')],
            ["Date of Birth:", self.user_info.get('dob', 'N/A')],
            ["Sample ID:", self.user_info.get('sample_id', 'N/A')],
            ["Report Date:", datetime.now().strftime("%Y-%m-%d")],
            ["Laboratory:", "Your Institution Name"],
        ]

        info_table = Table(info_data, colWidths=[2*inch, 4*inch])
        info_table.setStyle(TableStyle([
            ('FONT', (0, 0), (-1, -1), 'Helvetica', 10),
            ('FONT', (0, 0), (0, -1), 'Helvetica-Bold', 10),
            ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
            ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
            ('GRID', (0, 0), (-1, -1), 1, colors.grey),
        ]))

        elements.append(info_table)
        return elements

    def _create_executive_summary(self):
        """Create executive summary section"""
        elements = []

        elements.append(Paragraph("Executive Summary", self.styles['Heading1']))
        elements.append(Spacer(1, 0.2*inch))

        # Calculate statistics
        pathogenic = self.variants[
            self.variants['ACMG_Classification'].isin(['Pathogenic', 'Likely Pathogenic'])
        ]
        vus = self.variants[self.variants['ACMG_Classification'] == 'VUS']

        summary_text = f"""
        <b>Total Variants Analyzed:</b> {len(self.variants)}<br/>
        <b>Pathogenic/Likely Pathogenic:</b> {len(pathogenic)}<br/>
        <b>Variants of Uncertain Significance (VUS):</b> {len(vus)}<br/>
        <br/>
        <b>Clinical Interpretation:</b><br/>
        """

        if len(pathogenic) > 0:
            summary_text += """
            This analysis identified clinically significant variants that may be
            associated with disease risk. Genetic counseling is recommended to
            discuss implications and potential management strategies.
            """
        else:
            summary_text += """
            No pathogenic or likely pathogenic variants were identified in the
            analyzed exome regions. This does not exclude genetic etiology, as
            variants may exist in non-coding regions or genes not covered by
            this analysis.
            """

        elements.append(Paragraph(summary_text, self.styles['BodyText']))
        return elements

    def _create_pathogenic_section(self):
        """Create section for pathogenic variants"""
        elements = []

        elements.append(Paragraph("Pathogenic and Likely Pathogenic Variants",
                                 self.styles['Heading1']))
        elements.append(Spacer(1, 0.2*inch))

        pathogenic = self.variants[
            self.variants['ACMG_Classification'].isin(['Pathogenic', 'Likely Pathogenic'])
        ].head(20)  # Limit to 20 for space

        if len(pathogenic) == 0:
            elements.append(Paragraph("No pathogenic variants identified.",
                                    self.styles['BodyText']))
            return elements

        # Create variant table
        table_data = [['Gene', 'Variant', 'Classification', 'ClinVar', 'gnomAD AF']]

        for _, row in pathogenic.iterrows():
            table_data.append([
                row.get('Gene.refGene', 'N/A'),
                f"{row.get('AAChange.refGene', 'N/A')}",
                row.get('ACMG_Classification', 'N/A'),
                row.get('CLNSIG', 'N/A'),
                row.get('gnomad40_exome_AF', 'N/A')
            ])

        variant_table = Table(table_data, colWidths=[1.2*inch, 2*inch, 1.3*inch, 1.2*inch, 1*inch])
        variant_table.setStyle(TableStyle([
            ('FONT', (0, 0), (-1, 0), 'Helvetica-Bold', 9),
            ('FONT', (0, 1), (-1, -1), 'Helvetica', 8),
            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
            ('GRID', (0, 0), (-1, -1), 1, colors.black),
        ]))

        elements.append(variant_table)
        return elements

    def _create_vus_section(self):
        """Create VUS section"""
        elements = []

        elements.append(Spacer(1, 0.3*inch))
        elements.append(Paragraph("Variants of Uncertain Significance (VUS)",
                                 self.styles['Heading2']))

        vus = self.variants[self.variants['ACMG_Classification'] == 'VUS']

        vus_text = f"""
        {len(vus)} variants of uncertain significance were identified.
        These variants require additional evidence for clinical interpretation.
        Periodic re-evaluation is recommended as new data becomes available.
        """

        elements.append(Paragraph(vus_text, self.styles['BodyText']))
        return elements

    def _create_secondary_findings(self):
        """ACMG Secondary Findings section"""
        elements = []

        elements.append(PageBreak())
        elements.append(Paragraph("ACMG Secondary Findings", self.styles['Heading1']))

        acmg_genes = [
            'BRCA1', 'BRCA2', 'TP53', 'PTEN', 'MLH1', 'MSH2', 'MSH6', 'PMS2',
            'APC', 'RB1', 'LDLR', 'PCSK9', 'APOB', 'MYH7', 'MYBPC3', 'TTN'
            # ... (81 total ACMG SF v3.2 genes)
        ]

        secondary_findings = self.variants[
            self.variants['Gene.refGene'].isin(acmg_genes) &
            self.variants['ACMG_Classification'].isin(['Pathogenic', 'Likely Pathogenic'])
        ]

        if len(secondary_findings) == 0:
            elements.append(Paragraph(
                "No actionable secondary findings in ACMG SF v3.2 genes.",
                self.styles['BodyText']
            ))
        else:
            elements.append(Paragraph(
                f"‚ö†Ô∏è {len(secondary_findings)} secondary finding(s) identified in "
                "medically actionable genes. Genetic counseling strongly recommended.",
                self.styles['BodyText']
            ))

        return elements

    def _create_methodology(self):
        """Methodology section"""
        elements = []

        elements.append(PageBreak())
        elements.append(Paragraph("Methodology", self.styles['Heading1']))

        methodology_text = """
        <b>Sequencing Platform:</b> Illumina NovaSeq 6000<br/>
        <b>Library Preparation:</b> Exome capture (Agilent SureSelect v8)<br/>
        <b>Reference Genome:</b> GRCh38/hg38<br/>
        <b>Alignment:</b> BWA-MEM v0.7.17<br/>
        <b>Variant Calling:</b> GATK HaplotypeCaller v4.6.0<br/>
        <b>Annotation:</b> ANNOVAR (RefSeq, ClinVar, gnomAD, dbNSFP)<br/>
        <b>Classification:</b> ACMG/AMP Guidelines (2015)<br/>
        <br/>
        <b>Quality Thresholds:</b><br/>
        - Minimum Read Depth: 5x<br/>
        - Maximum Population Allele Frequency: 5%<br/>
        - Variant Quality Score Recalibration (VQSR) applied<br/>
        <br/>
        <b>Limitations:</b><br/>
        - Exome sequencing does not detect variants in non-coding regions<br/>
        - Structural variants (CNVs, inversions) may not be detected<br/>
        - Repeat expansions and mitochondrial variants not assessed<br/>
        - Clinical correlation and confirmatory testing recommended<br/>
        """

        elements.append(Paragraph(methodology_text, self.styles['BodyText']))
        return elements

# Example usage in endpoint
@app.get("/jobs/{job_id}/report/pdf")
async def generate_clinical_report(
    job_id: str,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Generate PDF clinical report"""

    # Load variants
    job = get_job(job_id)
    variants_df = pd.read_csv(job.filtered_tsv_path, sep='\t')

    # Generate report
    report_path = f"/tmp/{job_id}_clinical_report.pdf"
    generator = ClinicalReportGenerator(
        job_id=job_id,
        user_info={"name": user.username, "sample_id": job.sample_name},
        variants_df=variants_df
    )
    generator.generate_report(report_path)

    # Audit log
    AuditService.log(
        db, user.firebase_uid, user.email, "REPORT_GENERATED",
        resource_type="report", resource_id=job_id,
        data_exported=True
    )

    return FileResponse(report_path, filename=f"{job_id}_report.pdf")
```

**Install Dependencies:**
```bash
pip install reportlab pandas
```

**Impact:** Provides clinician-friendly reports, improves clinical utility

---

### 8. Add ACMG Secondary Findings Filtering

**Current Gap:** No systematic secondary findings reporting

**Solution - Create Secondary Findings Filter:**

Create `/backend/services/secondary_findings.py`:
```python
"""
ACMG Secondary Findings v3.2 (2023)
https://www.acmg.net/ACMG/Medical-Genetics-Practice-Resources/ACMG-SF-List.aspx
"""

ACMG_SF_GENES_V3_2 = {
    # Hereditary Breast/Ovarian Cancer
    "BRCA1": {"condition": "Breast/Ovarian Cancer", "action": "Surveillance/prophylaxis"},
    "BRCA2": {"condition": "Breast/Ovarian Cancer", "action": "Surveillance/prophylaxis"},
    "PALB2": {"condition": "Breast Cancer", "action": "Surveillance"},

    # Lynch Syndrome (Colorectal Cancer)
    "MLH1": {"condition": "Lynch Syndrome", "action": "Colonoscopy surveillance"},
    "MSH2": {"condition": "Lynch Syndrome", "action": "Colonoscopy surveillance"},
    "MSH6": {"condition": "Lynch Syndrome", "action": "Colonoscopy surveillance"},
    "PMS2": {"condition": "Lynch Syndrome", "action": "Colonoscopy surveillance"},
    "EPCAM": {"condition": "Lynch Syndrome", "action": "Colonoscopy surveillance"},

    # Li-Fraumeni Syndrome
    "TP53": {"condition": "Li-Fraumeni Syndrome", "action": "Enhanced surveillance"},

    # PTEN Hamartoma Tumor Syndrome
    "PTEN": {"condition": "Cowden Syndrome", "action": "Cancer surveillance"},

    # Familial Adenomatous Polyposis
    "APC": {"condition": "FAP", "action": "Prophylactic colectomy consideration"},

    # Retinoblastoma
    "RB1": {"condition": "Retinoblastoma", "action": "Ophthalmologic surveillance"},

    # Familial Hypercholesterolemia
    "LDLR": {"condition": "Familial Hypercholesterolemia", "action": "Lipid management"},
    "APOB": {"condition": "Familial Hypercholesterolemia", "action": "Lipid management"},
    "PCSK9": {"condition": "Familial Hypercholesterolemia", "action": "Lipid management"},

    # Cardiac Arrhythmias
    "KCNQ1": {"condition": "Long QT Syndrome", "action": "Cardiology evaluation"},
    "KCNH2": {"condition": "Long QT Syndrome", "action": "Cardiology evaluation"},
    "SCN5A": {"condition": "Brugada/Long QT", "action": "Cardiology evaluation"},
    "RYR2": {"condition": "CPVT", "action": "Beta-blocker therapy"},

    # Cardiomyopathies
    "MYH7": {"condition": "Hypertrophic Cardiomyopathy", "action": "Echo surveillance"},
    "MYBPC3": {"condition": "Hypertrophic Cardiomyopathy", "action": "Echo surveillance"},
    "TTN": {"condition": "Dilated Cardiomyopathy", "action": "Cardiology follow-up"},
    "LMNA": {"condition": "Dilated Cardiomyopathy", "action": "Cardiology follow-up"},
    "DSP": {"condition": "Arrhythmogenic Cardiomyopathy", "action": "ICD consideration"},
    "PKP2": {"condition": "Arrhythmogenic Cardiomyopathy", "action": "ICD consideration"},

    # Aortopathies
    "FBN1": {"condition": "Marfan Syndrome", "action": "Aortic surveillance"},
    "TGFBR1": {"condition": "Loeys-Dietz Syndrome", "action": "Aortic surveillance"},
    "TGFBR2": {"condition": "Loeys-Dietz Syndrome", "action": "Aortic surveillance"},
    "SMAD3": {"condition": "Loeys-Dietz Syndrome", "action": "Aortic surveillance"},
    "ACTA2": {"condition": "Familial Thoracic Aortic Aneurysm", "action": "Imaging"},
    "MYLK": {"condition": "Familial Thoracic Aortic Aneurysm", "action": "Imaging"},
    "MYH11": {"condition": "Familial Thoracic Aortic Aneurysm", "action": "Imaging"},

    # Malignant Hyperthermia
    "RYR1": {"condition": "Malignant Hyperthermia", "action": "Avoid triggering anesthetics"},
    "CACNA1S": {"condition": "Malignant Hyperthermia", "action": "Avoid triggering anesthetics"},

    # Von Hippel-Lindau
    "VHL": {"condition": "Von Hippel-Lindau", "action": "Tumor surveillance"},

    # Multiple Endocrine Neoplasia
    "MEN1": {"condition": "MEN1", "action": "Endocrine surveillance"},
    "RET": {"condition": "MEN2", "action": "Prophylactic thyroidectomy"},

    # Tuberous Sclerosis
    "TSC1": {"condition": "Tuberous Sclerosis", "action": "Multi-organ surveillance"},
    "TSC2": {"condition": "Tuberous Sclerosis", "action": "Multi-organ surveillance"},

    # Neurofibromatosis
    "NF2": {"condition": "Neurofibromatosis 2", "action": "Neurological surveillance"},

    # Hereditary Paraganglioma-Pheochromocytoma
    "SDHD": {"condition": "Paraganglioma", "action": "Biochemical/imaging surveillance"},
    "SDHAF2": {"condition": "Paraganglioma", "action": "Biochemical/imaging surveillance"},
    "SDHC": {"condition": "Paraganglioma", "action": "Biochemical/imaging surveillance"},
    "SDHB": {"condition": "Paraganglioma/Pheo", "action": "Biochemical/imaging surveillance"},
    "MAX": {"condition": "Pheochromocytoma", "action": "Biochemical surveillance"},
    "TMEM127": {"condition": "Pheochromocytoma", "action": "Biochemical surveillance"},

    # Other
    "WT1": {"condition": "Wilms Tumor", "action": "Renal ultrasound"},
    "OTC": {"condition": "Ornithine Transcarbamylase Deficiency", "action": "Protein restriction"},
    "ATP7B": {"condition": "Wilson Disease", "action": "Copper management"},

    # Additional genes from latest update
    "COL3A1": {"condition": "Vascular Ehlers-Danlos", "action": "Vascular imaging"},
    "GLA": {"condition": "Fabry Disease", "action": "Enzyme replacement"},
    "KCNE1": {"condition": "Long QT Syndrome", "action": "Cardiology evaluation"},
    "KCNE2": {"condition": "Long QT Syndrome", "action": "Cardiology evaluation"},
    "PRKAG2": {"condition": "Glycogen Storage Cardiomyopathy", "action": "Cardiology"},
    "PTEN": {"condition": "PTEN Hamartoma", "action": "Cancer surveillance"},
    # ... (81 total genes)
}

def filter_secondary_findings(variants_df: pd.DataFrame) -> pd.DataFrame:
    """Filter for ACMG Secondary Findings"""

    # Filter for SF genes
    sf_variants = variants_df[
        variants_df['Gene.refGene'].isin(ACMG_SF_GENES_V3_2.keys())
    ].copy()

    # Add SF annotations
    sf_variants['SF_Condition'] = sf_variants['Gene.refGene'].map(
        lambda x: ACMG_SF_GENES_V3_2.get(x, {}).get('condition', '')
    )
    sf_variants['SF_Action'] = sf_variants['Gene.refGene'].map(
        lambda x: ACMG_SF_GENES_V3_2.get(x, {}).get('action', '')
    )

    # Only report P/LP variants
    actionable = sf_variants[
        sf_variants['ACMG_Classification'].isin(['Pathogenic', 'Likely Pathogenic'])
    ]

    return actionable

# Add endpoint
@app.get("/jobs/{job_id}/secondary-findings")
async def get_secondary_findings(
    job_id: str,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Get ACMG Secondary Findings"""

    job = get_job(job_id)
    variants_df = pd.read_csv(job.filtered_tsv_path, sep='\t')

    sf_variants = filter_secondary_findings(variants_df)

    # Audit log
    AuditService.log(
        db, user.firebase_uid, user.email, "SECONDARY_FINDINGS_ACCESSED",
        resource_id=job_id, phi_accessed=True
    )

    return {
        "count": len(sf_variants),
        "variants": sf_variants.to_dict('records')
    }
```

**Impact:** Identifies actionable incidental findings per ACMG guidelines

---

### 9. Implement Data Retention and Deletion Policies

**Requirement:** HIPAA requires data retention policies and secure deletion

**Solution - Create Data Management Service:**

Create `/backend/services/data_retention.py`:
```python
from datetime import datetime, timedelta
from pathlib import Path
import shutil
import subprocess
from database import SessionLocal
from models.jobs import Job, JobStatus

class DataRetentionService:

    DEFAULT_RETENTION_DAYS = 90  # 3 months

    @staticmethod
    def mark_for_deletion(job_id: str, db):
        """Mark job data for deletion"""
        job = db.query(Job).filter(Job.job_id == job_id).first()
        if not job:
            raise ValueError(f"Job {job_id} not found")

        job.deletion_scheduled = datetime.utcnow() + timedelta(days=7)
        db.commit()

        # Audit log
        from services.audit_service import AuditService
        AuditService.log(
            db, job.user_id, "", "DATA_DELETION_SCHEDULED",
            resource_id=job_id,
            details={"scheduled_date": job.deletion_scheduled.isoformat()}
        )

    @staticmethod
    def secure_delete_file(file_path: str):
        """Securely delete file (3-pass overwrite)"""
        if not Path(file_path).exists():
            return

        try:
            # Use shred for secure deletion (Linux)
            subprocess.run([
                'shred', '-vfz', '-n', '3', str(file_path)
            ], check=True, capture_output=True)
        except (FileNotFoundError, subprocess.CalledProcessError):
            # Fallback to standard deletion
            Path(file_path).unlink()

    @staticmethod
    def delete_job_data(job_id: str, db):
        """Permanently delete all job data"""
        job = db.query(Job).filter(Job.job_id == job_id).first()
        if not job:
            return

        # Collect all file paths
        files_to_delete = [
            job.fastq_r1_path,
            job.fastq_r2_path,
            job.bam_path,
            job.raw_vcf_path,
            job.annotated_vcf_path,
            job.filtered_tsv_path
        ]

        # Delete job directory
        job_dir = Path(f"results/{job_id}")
        if job_dir.exists():
            shutil.rmtree(job_dir, ignore_errors=True)

        # Secure delete individual files
        for file_path in files_to_delete:
            if file_path:
                DataRetentionService.secure_delete_file(file_path)

        # Update database
        job.status = JobStatus.DELETED
        job.deleted_at = datetime.utcnow()
        db.commit()

        # Audit log
        from services.audit_service import AuditService
        AuditService.log(
            db, job.user_id, "", "DATA_DELETED",
            resource_id=job_id,
            details={"files_deleted": len([f for f in files_to_delete if f])},
            data_exported=False
        )

    @staticmethod
    def cleanup_expired_jobs():
        """Background task to delete expired job data"""
        db = SessionLocal()

        try:
            cutoff_date = datetime.utcnow() - timedelta(
                days=DataRetentionService.DEFAULT_RETENTION_DAYS
            )

            # Find jobs older than retention period
            expired_jobs = db.query(Job).filter(
                Job.completed_at < cutoff_date,
                Job.status != JobStatus.DELETED
            ).all()

            for job in expired_jobs:
                print(f"Deleting expired job: {job.job_id}")
                DataRetentionService.delete_job_data(job.job_id, db)

        finally:
            db.close()

# Add to database models (models/jobs.py)
class Job(Base):
    # ... existing fields ...

    deletion_scheduled = Column(DateTime, nullable=True)
    deleted_at = Column(DateTime, nullable=True)

# Add endpoint for user-initiated deletion
@app.delete("/jobs/{job_id}")
async def delete_job(
    job_id: str,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Delete job data permanently"""

    job = get_job(job_id)

    # Verify ownership
    if job.user_id != user.id:
        raise HTTPException(403, "Not authorized")

    # Immediate deletion
    DataRetentionService.delete_job_data(job_id, db)

    return {"message": "Job data deleted successfully"}

# Add scheduled cleanup task (run daily)
from apscheduler.schedulers.background import BackgroundScheduler

scheduler = BackgroundScheduler()
scheduler.add_job(
    DataRetentionService.cleanup_expired_jobs,
    'cron',
    hour=2,  # Run at 2 AM daily
    minute=0
)
scheduler.start()
```

**Install Dependencies:**
```bash
pip install apscheduler
```

**Impact:** HIPAA compliance, automatic cleanup, user control

---

### 10. Add Reference Sample Validation

**Current Gap:** No validation against known reference samples

**Solution - Add NA12878 Control Sample:**

Create `/processes/09_validation.nf`:
```groovy
process validateWithReference {
    tag "$sample_id"

    publishDir "${params.output_dir}/QC/concordance", mode: 'copy'

    when:
        params.validate_with_reference == true

    input:
        tuple val(sample_id), path(sample_vcf)
        path(reference_vcf)  // NA12878 truth set

    output:
        tuple val(sample_id), path("${sample_id}.concordance.txt")

    script:
    """
    # Compare against NA12878 truth set using bcftools
    bcftools isec \\
        -p isec_output \\
        ${sample_vcf} \\
        ${reference_vcf}

    # Calculate concordance metrics
    SAMPLE_ONLY=\$(bcftools view -H isec_output/0000.vcf | wc -l)
    REFERENCE_ONLY=\$(bcftools view -H isec_output/0001.vcf | wc -l)
    SHARED=\$(bcftools view -H isec_output/0002.vcf | wc -l)

    TOTAL=\$((SAMPLE_ONLY + SHARED))
    CONCORDANCE=\$(echo "scale=4; \$SHARED / \$TOTAL * 100" | bc)

    # Report
    echo "Sample: ${sample_id}" > ${sample_id}.concordance.txt
    echo "Reference: NA12878" >> ${sample_id}.concordance.txt
    echo "Shared Variants: \$SHARED" >> ${sample_id}.concordance.txt
    echo "Sample-Only: \$SAMPLE_ONLY" >> ${sample_id}.concordance.txt
    echo "Reference-Only: \$REFERENCE_ONLY" >> ${sample_id}.concordance.txt
    echo "Concordance: \${CONCORDANCE}%" >> ${sample_id}.concordance.txt

    # Validate > 95% concordance for control samples
    if (( \$(echo "\$CONCORDANCE > 95" | bc -l) )); then
        echo "‚úì PASS: Concordance > 95%"
    else
        echo "‚ö† WARNING: Low concordance (\${CONCORDANCE}%)"
    fi
    """
}
```

**Add to nextflow.config:**
```groovy
params {
    validate_with_reference = false
    na12878_vcf = "/path/to/NA12878_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-Solid_CHROM1-22_v3.3.2_highconf.vcf.gz"
}
```

**Impact:** Quality assurance, pipeline validation, error detection

---

## üü¢ MEDIUM PRIORITY IMPROVEMENTS

### 11. Environment Variables for System Paths

**Update nextflow.config:**
```groovy
params {
    reference = System.getenv('WES_REFERENCE') ?: '/media/drprabudh/m1/hg38/hg38.fa'
    annovar_dir = System.getenv('ANNOVAR_DIR') ?: '/media/drprabudh/m1/annovar'
    annovar_db = System.getenv('ANNOVAR_DB') ?: '/media/drprabudh/m1/annovar/hg38_humandb'
    intervals = System.getenv('WES_INTERVALS') ?: '/media/drprabudh/m1/Downloads/WES_Final_Agilent_V8_Hg38_1.bed'
}
```

**Create Environment Setup Script:**
```bash
# File: setup_environment.sh
#!/bin/bash

export WES_REFERENCE="/media/drprabudh/m1/hg38/hg38.fa"
export ANNOVAR_DIR="/media/drprabudh/m1/annovar"
export ANNOVAR_DB="/media/drprabudh/m1/annovar/hg38_humandb"
export WES_INTERVALS="/media/drprabudh/m1/Downloads/WES_Final_Agilent_V8_Hg38_1.bed"
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/firebase-service-account.json"

echo "‚úì WES environment configured"
```

**Impact:** Improved portability, easier deployment

---

### 12. Add Monitoring Dashboard

**Create `/backend/routes/monitoring.py`:**
```python
from fastapi import APIRouter, Depends
from database import get_db
from models.jobs import Job, JobStatus
from sqlalchemy import func
from datetime import datetime, timedelta

router = APIRouter(prefix="/monitoring", tags=["monitoring"])

@router.get("/dashboard")
async def get_dashboard_metrics(db: Session = Depends(get_db)):
    """Get system dashboard metrics"""

    now = datetime.utcnow()
    last_24h = now - timedelta(hours=24)
    last_7d = now - timedelta(days=7)

    metrics = {
        "jobs": {
            "total": db.query(Job).count(),
            "pending": db.query(Job).filter(Job.status == JobStatus.PENDING).count(),
            "running": db.query(Job).filter(Job.status == JobStatus.RUNNING).count(),
            "completed": db.query(Job).filter(Job.status == JobStatus.COMPLETED).count(),
            "failed": db.query(Job).filter(Job.status == JobStatus.FAILED).count(),
        },
        "activity": {
            "last_24h": db.query(Job).filter(Job.created_at >= last_24h).count(),
            "last_7d": db.query(Job).filter(Job.created_at >= last_7d).count(),
        },
        "performance": {
            "avg_runtime_hours": db.query(
                func.avg(
                    func.julianday(Job.completed_at) - func.julianday(Job.started_at)
                ) * 24
            ).filter(Job.status == JobStatus.COMPLETED).scalar() or 0,
        },
        "users": {
            "total": db.query(User).count(),
            "active_7d": db.query(User).join(Job).filter(
                Job.created_at >= last_7d
            ).distinct().count(),
        }
    }

    return metrics

@router.get("/jobs/running")
async def get_running_jobs(db: Session = Depends(get_db)):
    """Get currently running jobs with progress"""

    running = db.query(Job).filter(Job.status == JobStatus.RUNNING).all()

    return [{
        "job_id": job.job_id,
        "sample_name": job.sample_name,
        "current_step": job.current_step,
        "started_at": job.started_at.isoformat(),
        "runtime_hours": (datetime.utcnow() - job.started_at).total_seconds() / 3600
    } for job in running]
```

**Impact:** Real-time monitoring, performance tracking

---

### 13. Implement Email Notifications

**Already implemented** in `/backend/services/email_service.py`

**Enhancement - Add More Notification Types:**
```python
class EmailTemplates:

    @staticmethod
    def job_completed(job_id: str, sample_name: str):
        return f"""
        <h2>Analysis Complete</h2>
        <p>Your whole exome sequencing analysis has completed successfully.</p>
        <p><b>Sample:</b> {sample_name}</p>
        <p><b>Job ID:</b> {job_id}</p>
        <p><a href="https://your-platform.com/jobs/{job_id}">View Results</a></p>
        """

    @staticmethod
    def pathogenic_variant_found(job_id: str, variant_count: int):
        return f"""
        <h2>‚ö†Ô∏è Pathogenic Variants Detected</h2>
        <p><b>{variant_count}</b> pathogenic or likely pathogenic variant(s) identified.</p>
        <p><b>Job ID:</b> {job_id}</p>
        <p>Genetic counseling is recommended.</p>
        <p><a href="https://your-platform.com/jobs/{job_id}/report">View Report</a></p>
        """

    @staticmethod
    def secondary_finding_alert(job_id: str, gene: str, condition: str):
        return f"""
        <h2>üîî ACMG Secondary Finding</h2>
        <p>An actionable secondary finding was identified:</p>
        <p><b>Gene:</b> {gene}</p>
        <p><b>Condition:</b> {condition}</p>
        <p><b>Job ID:</b> {job_id}</p>
        <p>Please review and discuss with genetic counselor.</p>
        """
```

**Impact:** Improves user engagement, timely notifications

---

## üìã REGULATORY COMPLIANCE CHECKLIST

### HIPAA Compliance
- ‚úÖ Encryption in transit (HTTPS)
- ‚ö†Ô∏è **Needed:** Encryption at rest (database, files)
- ‚ö†Ô∏è **Needed:** Business Associate Agreement (BAA)
- ‚ö†Ô∏è **Needed:** PHI access logging (implement audit logging above)
- ‚ö†Ô∏è **Needed:** User consent management
- ‚ö†Ô∏è **Needed:** Breach notification procedures
- ‚ö†Ô∏è **Needed:** Regular security audits

### CLIA/CAP Accreditation
- ‚ö†Ô∏è **Needed:** Standard Operating Procedures (SOPs)
- ‚ö†Ô∏è **Needed:** Quality Control documentation
- ‚ö†Ô∏è **Needed:** Proficiency testing
- ‚ö†Ô∏è **Needed:** Result verification process
- ‚ö†Ô∏è **Needed:** Staff competency assessment
- ‚ö†Ô∏è **Needed:** Laboratory director oversight

### GDPR (if serving EU)
- ‚ö†Ô∏è **Needed:** Data residency controls
- ‚ö†Ô∏è **Needed:** Right to deletion (implement above)
- ‚ö†Ô∏è **Needed:** Data portability
- ‚ö†Ô∏è **Needed:** Privacy policy
- ‚ö†Ô∏è **Needed:** Cookie consent
- ‚ö†Ô∏è **Needed:** Data processing agreements

### FDA (if diagnostic claims)
- ‚ö†Ô∏è **Needed:** Clinical validation study
- ‚ö†Ô∏è **Needed:** Analytical validation
- ‚ö†Ô∏è **Needed:** 510(k) submission (if applicable)
- ‚ö†Ô∏è **Needed:** Quality management system

---

## üîß IMPLEMENTATION PRIORITY ROADMAP

### Phase 1: Critical Fixes (Week 1-2)
1. ‚úÖ Remove author attribution blocking
2. ‚úÖ Fix Firebase credentials security
3. ‚úÖ Implement FASTQ validation
4. ‚úÖ Add comprehensive audit logging

### Phase 2: Quality Improvements (Week 3-4)
5. ‚úÖ Implement VQSR
6. ‚úÖ Add contamination detection
7. ‚úÖ Clinical report generation
8. ‚úÖ ACMG secondary findings

### Phase 3: Compliance & Operations (Week 5-8)
9. ‚úÖ Data retention policies
10. ‚úÖ Reference sample validation
11. ‚úÖ Environment variable configuration
12. ‚úÖ Monitoring dashboard

### Phase 4: Regulatory Preparation (Week 9-12)
13. ‚ö†Ô∏è HIPAA compliance documentation
14. ‚ö†Ô∏è CLIA/CAP accreditation preparation
15. ‚ö†Ô∏è Clinical validation study
16. ‚ö†Ô∏è Quality management system

---

## üìä COST OPTIMIZATION RECOMMENDATIONS

### 1. Use Preemptible/Spot Instances
- For non-urgent analyses, use AWS Spot or GCP Preemptible VMs
- **Savings:** 60-80% compute cost reduction

### 2. Implement Caching
- Cache reference files in memory
- Store intermediate BAM files for rapid re-analysis
- **Savings:** 40% reduced re-run time

### 3. Storage Tiering
- Move completed jobs to cold storage after 30 days
- **Savings:** 70% storage cost reduction

### 4. Batch Processing
- Queue jobs and process in batches
- Better resource utilization
- **Savings:** 30% compute efficiency gain

---

## üéØ CONCLUSION

Your Nextflow WES pipeline has a **solid foundation** for clinical use. Implementing these recommendations will:

1. **Improve Clinical Utility** - Better reports, actionable findings
2. **Enhance Security** - HIPAA compliance, audit logging
3. **Increase Reliability** - Validation, quality control
4. **Enable Regulatory Approval** - Documentation, compliance

**Estimated Implementation Effort:** 8-12 weeks with 2-3 developers

**Next Steps:**
1. Prioritize critical fixes (Phase 1)
2. Engage regulatory consultant for compliance roadmap
3. Initiate clinical validation study
4. Develop comprehensive SOPs
5. Implement monitoring and alerting

For a SaaS genomics platform, these enhancements are **essential** for market competitiveness and regulatory approval.

---

**Document Version:** 1.0
**Last Updated:** 2026-01-14
**Maintained By:** Clinical Genomics Team
